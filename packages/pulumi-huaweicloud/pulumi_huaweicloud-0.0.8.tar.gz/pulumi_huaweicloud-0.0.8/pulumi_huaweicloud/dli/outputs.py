# coding=utf-8
# *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
# *** Do not edit by hand unless you're certain you know what you are doing! ***

import copy
import warnings
import pulumi
import pulumi.runtime
from typing import Any, Mapping, Optional, Sequence, Union, overload
from .. import _utilities
from . import outputs

__all__ = [
    'SparkJobDependentPackage',
    'SparkJobDependentPackagePackage',
    'SqlJobConf',
    'TableColumn',
]

@pulumi.output_type
class SparkJobDependentPackage(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "groupName":
            suggest = "group_name"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in SparkJobDependentPackage. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        SparkJobDependentPackage.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        SparkJobDependentPackage.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 group_name: str,
                 packages: Sequence['outputs.SparkJobDependentPackagePackage']):
        """
        :param str group_name: Specifies the user group name.
               Changing this parameter will submit a new spark job.
        :param Sequence['SparkJobDependentPackagePackageArgs'] packages: Specifies the user group resource for details.
               Changing this parameter will submit a new spark job.
               The object structure is documented below.
        """
        pulumi.set(__self__, "group_name", group_name)
        pulumi.set(__self__, "packages", packages)

    @property
    @pulumi.getter(name="groupName")
    def group_name(self) -> str:
        """
        Specifies the user group name.
        Changing this parameter will submit a new spark job.
        """
        return pulumi.get(self, "group_name")

    @property
    @pulumi.getter
    def packages(self) -> Sequence['outputs.SparkJobDependentPackagePackage']:
        """
        Specifies the user group resource for details.
        Changing this parameter will submit a new spark job.
        The object structure is documented below.
        """
        return pulumi.get(self, "packages")


@pulumi.output_type
class SparkJobDependentPackagePackage(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "packageName":
            suggest = "package_name"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in SparkJobDependentPackagePackage. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        SparkJobDependentPackagePackage.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        SparkJobDependentPackagePackage.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 package_name: str,
                 type: str):
        """
        :param str package_name: Specifies the resource name of the package.
               Changing this parameter will submit a new spark job.
        :param str type: Specifies the resource type of the package.
               Changing this parameter will submit a new spark job.
        """
        pulumi.set(__self__, "package_name", package_name)
        pulumi.set(__self__, "type", type)

    @property
    @pulumi.getter(name="packageName")
    def package_name(self) -> str:
        """
        Specifies the resource name of the package.
        Changing this parameter will submit a new spark job.
        """
        return pulumi.get(self, "package_name")

    @property
    @pulumi.getter
    def type(self) -> str:
        """
        Specifies the resource type of the package.
        Changing this parameter will submit a new spark job.
        """
        return pulumi.get(self, "type")


@pulumi.output_type
class SqlJobConf(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "dliSqlJobTimeout":
            suggest = "dli_sql_job_timeout"
        elif key == "dliSqlSqlasyncEnabled":
            suggest = "dli_sql_sqlasync_enabled"
        elif key == "sparkSqlAutoBroadcastJoinThreshold":
            suggest = "spark_sql_auto_broadcast_join_threshold"
        elif key == "sparkSqlBadRecordsPath":
            suggest = "spark_sql_bad_records_path"
        elif key == "sparkSqlDynamicPartitionOverwriteEnabled":
            suggest = "spark_sql_dynamic_partition_overwrite_enabled"
        elif key == "sparkSqlFilesMaxPartitionBytes":
            suggest = "spark_sql_files_max_partition_bytes"
        elif key == "sparkSqlMaxRecordsPerFile":
            suggest = "spark_sql_max_records_per_file"
        elif key == "sparkSqlShufflePartitions":
            suggest = "spark_sql_shuffle_partitions"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in SqlJobConf. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        SqlJobConf.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        SqlJobConf.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 dli_sql_job_timeout: Optional[int] = None,
                 dli_sql_sqlasync_enabled: Optional[bool] = None,
                 spark_sql_auto_broadcast_join_threshold: Optional[int] = None,
                 spark_sql_bad_records_path: Optional[str] = None,
                 spark_sql_dynamic_partition_overwrite_enabled: Optional[bool] = None,
                 spark_sql_files_max_partition_bytes: Optional[int] = None,
                 spark_sql_max_records_per_file: Optional[int] = None,
                 spark_sql_shuffle_partitions: Optional[int] = None):
        """
        :param int dli_sql_job_timeout: Sets the job running timeout interval. If the timeout interval
               expires, the job is canceled. Unit: `ms`. Changing this parameter will create a new resource.
        :param bool dli_sql_sqlasync_enabled: Specifies whether DDL and DCL statements are executed
               asynchronously. The value true indicates that asynchronous execution is enabled. Default value is `false`.
               Changing this parameter will create a new resource.
        :param int spark_sql_auto_broadcast_join_threshold: Maximum size of the table that
               displays all working nodes when a connection is executed. You can set this parameter to -1 to disable the display.
               Default value is `209715200`. Changing this parameter will create a new resource.
        :param str spark_sql_bad_records_path: Path of bad records. Changing this parameter will create
               a new resource.
        :param bool spark_sql_dynamic_partition_overwrite_enabled: In dynamic mode, Spark does not delete
               the previous partitions and only overwrites the partitions without data during execution. Default value is `false`.
               Changing this parameter will create a new resource.
        :param int spark_sql_files_max_partition_bytes: Maximum number of bytes to be packed into a
               single partition when a file is read. Default value is `134217728`. Changing this parameter will create a new
               resource.
        :param int spark_sql_max_records_per_file: Maximum number of records to be written
               into a single file. If the value is zero or negative, there is no limit. Default value is `0`.
               Changing this parameter will create a new resource.
        :param int spark_sql_shuffle_partitions: Default number of partitions used to filter
               data for join or aggregation. Default value is `4096`. Changing this parameter will create a new resource.
        """
        if dli_sql_job_timeout is not None:
            pulumi.set(__self__, "dli_sql_job_timeout", dli_sql_job_timeout)
        if dli_sql_sqlasync_enabled is not None:
            pulumi.set(__self__, "dli_sql_sqlasync_enabled", dli_sql_sqlasync_enabled)
        if spark_sql_auto_broadcast_join_threshold is not None:
            pulumi.set(__self__, "spark_sql_auto_broadcast_join_threshold", spark_sql_auto_broadcast_join_threshold)
        if spark_sql_bad_records_path is not None:
            pulumi.set(__self__, "spark_sql_bad_records_path", spark_sql_bad_records_path)
        if spark_sql_dynamic_partition_overwrite_enabled is not None:
            pulumi.set(__self__, "spark_sql_dynamic_partition_overwrite_enabled", spark_sql_dynamic_partition_overwrite_enabled)
        if spark_sql_files_max_partition_bytes is not None:
            pulumi.set(__self__, "spark_sql_files_max_partition_bytes", spark_sql_files_max_partition_bytes)
        if spark_sql_max_records_per_file is not None:
            pulumi.set(__self__, "spark_sql_max_records_per_file", spark_sql_max_records_per_file)
        if spark_sql_shuffle_partitions is not None:
            pulumi.set(__self__, "spark_sql_shuffle_partitions", spark_sql_shuffle_partitions)

    @property
    @pulumi.getter(name="dliSqlJobTimeout")
    def dli_sql_job_timeout(self) -> Optional[int]:
        """
        Sets the job running timeout interval. If the timeout interval
        expires, the job is canceled. Unit: `ms`. Changing this parameter will create a new resource.
        """
        return pulumi.get(self, "dli_sql_job_timeout")

    @property
    @pulumi.getter(name="dliSqlSqlasyncEnabled")
    def dli_sql_sqlasync_enabled(self) -> Optional[bool]:
        """
        Specifies whether DDL and DCL statements are executed
        asynchronously. The value true indicates that asynchronous execution is enabled. Default value is `false`.
        Changing this parameter will create a new resource.
        """
        return pulumi.get(self, "dli_sql_sqlasync_enabled")

    @property
    @pulumi.getter(name="sparkSqlAutoBroadcastJoinThreshold")
    def spark_sql_auto_broadcast_join_threshold(self) -> Optional[int]:
        """
        Maximum size of the table that
        displays all working nodes when a connection is executed. You can set this parameter to -1 to disable the display.
        Default value is `209715200`. Changing this parameter will create a new resource.
        """
        return pulumi.get(self, "spark_sql_auto_broadcast_join_threshold")

    @property
    @pulumi.getter(name="sparkSqlBadRecordsPath")
    def spark_sql_bad_records_path(self) -> Optional[str]:
        """
        Path of bad records. Changing this parameter will create
        a new resource.
        """
        return pulumi.get(self, "spark_sql_bad_records_path")

    @property
    @pulumi.getter(name="sparkSqlDynamicPartitionOverwriteEnabled")
    def spark_sql_dynamic_partition_overwrite_enabled(self) -> Optional[bool]:
        """
        In dynamic mode, Spark does not delete
        the previous partitions and only overwrites the partitions without data during execution. Default value is `false`.
        Changing this parameter will create a new resource.
        """
        return pulumi.get(self, "spark_sql_dynamic_partition_overwrite_enabled")

    @property
    @pulumi.getter(name="sparkSqlFilesMaxPartitionBytes")
    def spark_sql_files_max_partition_bytes(self) -> Optional[int]:
        """
        Maximum number of bytes to be packed into a
        single partition when a file is read. Default value is `134217728`. Changing this parameter will create a new
        resource.
        """
        return pulumi.get(self, "spark_sql_files_max_partition_bytes")

    @property
    @pulumi.getter(name="sparkSqlMaxRecordsPerFile")
    def spark_sql_max_records_per_file(self) -> Optional[int]:
        """
        Maximum number of records to be written
        into a single file. If the value is zero or negative, there is no limit. Default value is `0`.
        Changing this parameter will create a new resource.
        """
        return pulumi.get(self, "spark_sql_max_records_per_file")

    @property
    @pulumi.getter(name="sparkSqlShufflePartitions")
    def spark_sql_shuffle_partitions(self) -> Optional[int]:
        """
        Default number of partitions used to filter
        data for join or aggregation. Default value is `4096`. Changing this parameter will create a new resource.
        """
        return pulumi.get(self, "spark_sql_shuffle_partitions")


@pulumi.output_type
class TableColumn(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "isPartition":
            suggest = "is_partition"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in TableColumn. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        TableColumn.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        TableColumn.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 name: str,
                 type: str,
                 description: Optional[str] = None,
                 is_partition: Optional[bool] = None):
        """
        :param str name: Specifies the name of column. Changing this parameter will create a new
               resource.
        :param str type: Specifies data type of column. Changing this parameter will create a new
               resource.
        :param str description: Specifies the description of column. Changing this parameter will
               create a new resource.
        :param bool is_partition: Specifies whether the column is a partition column. The value
               `true` indicates a partition column, and the value false indicates a non-partition column. The default value
               is false. Changing this parameter will create a new resource.
        """
        pulumi.set(__self__, "name", name)
        pulumi.set(__self__, "type", type)
        if description is not None:
            pulumi.set(__self__, "description", description)
        if is_partition is not None:
            pulumi.set(__self__, "is_partition", is_partition)

    @property
    @pulumi.getter
    def name(self) -> str:
        """
        Specifies the name of column. Changing this parameter will create a new
        resource.
        """
        return pulumi.get(self, "name")

    @property
    @pulumi.getter
    def type(self) -> str:
        """
        Specifies data type of column. Changing this parameter will create a new
        resource.
        """
        return pulumi.get(self, "type")

    @property
    @pulumi.getter
    def description(self) -> Optional[str]:
        """
        Specifies the description of column. Changing this parameter will
        create a new resource.
        """
        return pulumi.get(self, "description")

    @property
    @pulumi.getter(name="isPartition")
    def is_partition(self) -> Optional[bool]:
        """
        Specifies whether the column is a partition column. The value
        `true` indicates a partition column, and the value false indicates a non-partition column. The default value
        is false. Changing this parameter will create a new resource.
        """
        return pulumi.get(self, "is_partition")


