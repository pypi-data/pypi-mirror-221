# coding=utf-8
# *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
# *** Do not edit by hand unless you're certain you know what you are doing! ***

import copy
import warnings
import pulumi
import pulumi.runtime
from typing import Any, Mapping, Optional, Sequence, Union, overload
from .. import _utilities

__all__ = [
    'SparkJobDependentPackageArgs',
    'SparkJobDependentPackagePackageArgs',
    'SqlJobConfArgs',
    'TableColumnArgs',
]

@pulumi.input_type
class SparkJobDependentPackageArgs:
    def __init__(__self__, *,
                 group_name: pulumi.Input[str],
                 packages: pulumi.Input[Sequence[pulumi.Input['SparkJobDependentPackagePackageArgs']]]):
        """
        :param pulumi.Input[str] group_name: Specifies the user group name.
               Changing this parameter will submit a new spark job.
        :param pulumi.Input[Sequence[pulumi.Input['SparkJobDependentPackagePackageArgs']]] packages: Specifies the user group resource for details.
               Changing this parameter will submit a new spark job.
               The object structure is documented below.
        """
        pulumi.set(__self__, "group_name", group_name)
        pulumi.set(__self__, "packages", packages)

    @property
    @pulumi.getter(name="groupName")
    def group_name(self) -> pulumi.Input[str]:
        """
        Specifies the user group name.
        Changing this parameter will submit a new spark job.
        """
        return pulumi.get(self, "group_name")

    @group_name.setter
    def group_name(self, value: pulumi.Input[str]):
        pulumi.set(self, "group_name", value)

    @property
    @pulumi.getter
    def packages(self) -> pulumi.Input[Sequence[pulumi.Input['SparkJobDependentPackagePackageArgs']]]:
        """
        Specifies the user group resource for details.
        Changing this parameter will submit a new spark job.
        The object structure is documented below.
        """
        return pulumi.get(self, "packages")

    @packages.setter
    def packages(self, value: pulumi.Input[Sequence[pulumi.Input['SparkJobDependentPackagePackageArgs']]]):
        pulumi.set(self, "packages", value)


@pulumi.input_type
class SparkJobDependentPackagePackageArgs:
    def __init__(__self__, *,
                 package_name: pulumi.Input[str],
                 type: pulumi.Input[str]):
        """
        :param pulumi.Input[str] package_name: Specifies the resource name of the package.
               Changing this parameter will submit a new spark job.
        :param pulumi.Input[str] type: Specifies the resource type of the package.
               Changing this parameter will submit a new spark job.
        """
        pulumi.set(__self__, "package_name", package_name)
        pulumi.set(__self__, "type", type)

    @property
    @pulumi.getter(name="packageName")
    def package_name(self) -> pulumi.Input[str]:
        """
        Specifies the resource name of the package.
        Changing this parameter will submit a new spark job.
        """
        return pulumi.get(self, "package_name")

    @package_name.setter
    def package_name(self, value: pulumi.Input[str]):
        pulumi.set(self, "package_name", value)

    @property
    @pulumi.getter
    def type(self) -> pulumi.Input[str]:
        """
        Specifies the resource type of the package.
        Changing this parameter will submit a new spark job.
        """
        return pulumi.get(self, "type")

    @type.setter
    def type(self, value: pulumi.Input[str]):
        pulumi.set(self, "type", value)


@pulumi.input_type
class SqlJobConfArgs:
    def __init__(__self__, *,
                 dli_sql_job_timeout: Optional[pulumi.Input[int]] = None,
                 dli_sql_sqlasync_enabled: Optional[pulumi.Input[bool]] = None,
                 spark_sql_auto_broadcast_join_threshold: Optional[pulumi.Input[int]] = None,
                 spark_sql_bad_records_path: Optional[pulumi.Input[str]] = None,
                 spark_sql_dynamic_partition_overwrite_enabled: Optional[pulumi.Input[bool]] = None,
                 spark_sql_files_max_partition_bytes: Optional[pulumi.Input[int]] = None,
                 spark_sql_max_records_per_file: Optional[pulumi.Input[int]] = None,
                 spark_sql_shuffle_partitions: Optional[pulumi.Input[int]] = None):
        """
        :param pulumi.Input[int] dli_sql_job_timeout: Sets the job running timeout interval. If the timeout interval
               expires, the job is canceled. Unit: `ms`. Changing this parameter will create a new resource.
        :param pulumi.Input[bool] dli_sql_sqlasync_enabled: Specifies whether DDL and DCL statements are executed
               asynchronously. The value true indicates that asynchronous execution is enabled. Default value is `false`.
               Changing this parameter will create a new resource.
        :param pulumi.Input[int] spark_sql_auto_broadcast_join_threshold: Maximum size of the table that
               displays all working nodes when a connection is executed. You can set this parameter to -1 to disable the display.
               Default value is `209715200`. Changing this parameter will create a new resource.
        :param pulumi.Input[str] spark_sql_bad_records_path: Path of bad records. Changing this parameter will create
               a new resource.
        :param pulumi.Input[bool] spark_sql_dynamic_partition_overwrite_enabled: In dynamic mode, Spark does not delete
               the previous partitions and only overwrites the partitions without data during execution. Default value is `false`.
               Changing this parameter will create a new resource.
        :param pulumi.Input[int] spark_sql_files_max_partition_bytes: Maximum number of bytes to be packed into a
               single partition when a file is read. Default value is `134217728`. Changing this parameter will create a new
               resource.
        :param pulumi.Input[int] spark_sql_max_records_per_file: Maximum number of records to be written
               into a single file. If the value is zero or negative, there is no limit. Default value is `0`.
               Changing this parameter will create a new resource.
        :param pulumi.Input[int] spark_sql_shuffle_partitions: Default number of partitions used to filter
               data for join or aggregation. Default value is `4096`. Changing this parameter will create a new resource.
        """
        if dli_sql_job_timeout is not None:
            pulumi.set(__self__, "dli_sql_job_timeout", dli_sql_job_timeout)
        if dli_sql_sqlasync_enabled is not None:
            pulumi.set(__self__, "dli_sql_sqlasync_enabled", dli_sql_sqlasync_enabled)
        if spark_sql_auto_broadcast_join_threshold is not None:
            pulumi.set(__self__, "spark_sql_auto_broadcast_join_threshold", spark_sql_auto_broadcast_join_threshold)
        if spark_sql_bad_records_path is not None:
            pulumi.set(__self__, "spark_sql_bad_records_path", spark_sql_bad_records_path)
        if spark_sql_dynamic_partition_overwrite_enabled is not None:
            pulumi.set(__self__, "spark_sql_dynamic_partition_overwrite_enabled", spark_sql_dynamic_partition_overwrite_enabled)
        if spark_sql_files_max_partition_bytes is not None:
            pulumi.set(__self__, "spark_sql_files_max_partition_bytes", spark_sql_files_max_partition_bytes)
        if spark_sql_max_records_per_file is not None:
            pulumi.set(__self__, "spark_sql_max_records_per_file", spark_sql_max_records_per_file)
        if spark_sql_shuffle_partitions is not None:
            pulumi.set(__self__, "spark_sql_shuffle_partitions", spark_sql_shuffle_partitions)

    @property
    @pulumi.getter(name="dliSqlJobTimeout")
    def dli_sql_job_timeout(self) -> Optional[pulumi.Input[int]]:
        """
        Sets the job running timeout interval. If the timeout interval
        expires, the job is canceled. Unit: `ms`. Changing this parameter will create a new resource.
        """
        return pulumi.get(self, "dli_sql_job_timeout")

    @dli_sql_job_timeout.setter
    def dli_sql_job_timeout(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "dli_sql_job_timeout", value)

    @property
    @pulumi.getter(name="dliSqlSqlasyncEnabled")
    def dli_sql_sqlasync_enabled(self) -> Optional[pulumi.Input[bool]]:
        """
        Specifies whether DDL and DCL statements are executed
        asynchronously. The value true indicates that asynchronous execution is enabled. Default value is `false`.
        Changing this parameter will create a new resource.
        """
        return pulumi.get(self, "dli_sql_sqlasync_enabled")

    @dli_sql_sqlasync_enabled.setter
    def dli_sql_sqlasync_enabled(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "dli_sql_sqlasync_enabled", value)

    @property
    @pulumi.getter(name="sparkSqlAutoBroadcastJoinThreshold")
    def spark_sql_auto_broadcast_join_threshold(self) -> Optional[pulumi.Input[int]]:
        """
        Maximum size of the table that
        displays all working nodes when a connection is executed. You can set this parameter to -1 to disable the display.
        Default value is `209715200`. Changing this parameter will create a new resource.
        """
        return pulumi.get(self, "spark_sql_auto_broadcast_join_threshold")

    @spark_sql_auto_broadcast_join_threshold.setter
    def spark_sql_auto_broadcast_join_threshold(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "spark_sql_auto_broadcast_join_threshold", value)

    @property
    @pulumi.getter(name="sparkSqlBadRecordsPath")
    def spark_sql_bad_records_path(self) -> Optional[pulumi.Input[str]]:
        """
        Path of bad records. Changing this parameter will create
        a new resource.
        """
        return pulumi.get(self, "spark_sql_bad_records_path")

    @spark_sql_bad_records_path.setter
    def spark_sql_bad_records_path(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "spark_sql_bad_records_path", value)

    @property
    @pulumi.getter(name="sparkSqlDynamicPartitionOverwriteEnabled")
    def spark_sql_dynamic_partition_overwrite_enabled(self) -> Optional[pulumi.Input[bool]]:
        """
        In dynamic mode, Spark does not delete
        the previous partitions and only overwrites the partitions without data during execution. Default value is `false`.
        Changing this parameter will create a new resource.
        """
        return pulumi.get(self, "spark_sql_dynamic_partition_overwrite_enabled")

    @spark_sql_dynamic_partition_overwrite_enabled.setter
    def spark_sql_dynamic_partition_overwrite_enabled(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "spark_sql_dynamic_partition_overwrite_enabled", value)

    @property
    @pulumi.getter(name="sparkSqlFilesMaxPartitionBytes")
    def spark_sql_files_max_partition_bytes(self) -> Optional[pulumi.Input[int]]:
        """
        Maximum number of bytes to be packed into a
        single partition when a file is read. Default value is `134217728`. Changing this parameter will create a new
        resource.
        """
        return pulumi.get(self, "spark_sql_files_max_partition_bytes")

    @spark_sql_files_max_partition_bytes.setter
    def spark_sql_files_max_partition_bytes(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "spark_sql_files_max_partition_bytes", value)

    @property
    @pulumi.getter(name="sparkSqlMaxRecordsPerFile")
    def spark_sql_max_records_per_file(self) -> Optional[pulumi.Input[int]]:
        """
        Maximum number of records to be written
        into a single file. If the value is zero or negative, there is no limit. Default value is `0`.
        Changing this parameter will create a new resource.
        """
        return pulumi.get(self, "spark_sql_max_records_per_file")

    @spark_sql_max_records_per_file.setter
    def spark_sql_max_records_per_file(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "spark_sql_max_records_per_file", value)

    @property
    @pulumi.getter(name="sparkSqlShufflePartitions")
    def spark_sql_shuffle_partitions(self) -> Optional[pulumi.Input[int]]:
        """
        Default number of partitions used to filter
        data for join or aggregation. Default value is `4096`. Changing this parameter will create a new resource.
        """
        return pulumi.get(self, "spark_sql_shuffle_partitions")

    @spark_sql_shuffle_partitions.setter
    def spark_sql_shuffle_partitions(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "spark_sql_shuffle_partitions", value)


@pulumi.input_type
class TableColumnArgs:
    def __init__(__self__, *,
                 name: pulumi.Input[str],
                 type: pulumi.Input[str],
                 description: Optional[pulumi.Input[str]] = None,
                 is_partition: Optional[pulumi.Input[bool]] = None):
        """
        :param pulumi.Input[str] name: Specifies the name of column. Changing this parameter will create a new
               resource.
        :param pulumi.Input[str] type: Specifies data type of column. Changing this parameter will create a new
               resource.
        :param pulumi.Input[str] description: Specifies the description of column. Changing this parameter will
               create a new resource.
        :param pulumi.Input[bool] is_partition: Specifies whether the column is a partition column. The value
               `true` indicates a partition column, and the value false indicates a non-partition column. The default value
               is false. Changing this parameter will create a new resource.
        """
        pulumi.set(__self__, "name", name)
        pulumi.set(__self__, "type", type)
        if description is not None:
            pulumi.set(__self__, "description", description)
        if is_partition is not None:
            pulumi.set(__self__, "is_partition", is_partition)

    @property
    @pulumi.getter
    def name(self) -> pulumi.Input[str]:
        """
        Specifies the name of column. Changing this parameter will create a new
        resource.
        """
        return pulumi.get(self, "name")

    @name.setter
    def name(self, value: pulumi.Input[str]):
        pulumi.set(self, "name", value)

    @property
    @pulumi.getter
    def type(self) -> pulumi.Input[str]:
        """
        Specifies data type of column. Changing this parameter will create a new
        resource.
        """
        return pulumi.get(self, "type")

    @type.setter
    def type(self, value: pulumi.Input[str]):
        pulumi.set(self, "type", value)

    @property
    @pulumi.getter
    def description(self) -> Optional[pulumi.Input[str]]:
        """
        Specifies the description of column. Changing this parameter will
        create a new resource.
        """
        return pulumi.get(self, "description")

    @description.setter
    def description(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "description", value)

    @property
    @pulumi.getter(name="isPartition")
    def is_partition(self) -> Optional[pulumi.Input[bool]]:
        """
        Specifies whether the column is a partition column. The value
        `true` indicates a partition column, and the value false indicates a non-partition column. The default value
        is false. Changing this parameter will create a new resource.
        """
        return pulumi.get(self, "is_partition")

    @is_partition.setter
    def is_partition(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "is_partition", value)


