Metadata-Version: 2.1
Name: im_openai
Version: 0.7.2
Summary: Wrapper library for openai to send events to the Imaginary Programming monitor
Home-page: https://github.com/alecf/im_openai
Author: Alec Flett
Author-email: alec@thegp.com
License: MIT license
Keywords: im_openai
Classifier: Development Status :: 2 - Pre-Alpha
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Natural Language :: English
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.6
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Requires-Python: >=3.6
Description-Content-Type: text/markdown
License-File: LICENSE
License-File: AUTHORS.rst

# Imaginary Dev OpenAI wrapper

[![image](https://img.shields.io/pypi/v/im_openai.svg)](https://pypi.python.org/pypi/im_openai)

[![image](https://img.shields.io/travis/alecf/im_openai.svg)](https://travis-ci.com/alecf/im_openai)

[![Documentation Status](https://readthedocs.org/projects/im-openai/badge/?version=latest)](https://im-openai.readthedocs.io/en/latest/?version=latest)

Wrapper library for openai to send events to the Imaginary Programming
monitor

-   Free software: MIT license
-   Documentation: <https://im-openai.readthedocs.io>.

## Features

-   Patches the openai library to allow user to set an ip_project_key
    for each request
-   Works out of the box with langchain

## Get Started

At startup, before any openai calls, patch the library with the
following code:

```python
from im_openai import patch_openai
patch_openai()
```

Then, set the ip_project_key for each request:

```python
import openai

completion = openai.ChatCompletion.create(
    engine="davinci",
    prompt="This is a test",
    ip_project_key="my_project_key"
)
```

If you're using langchain, you can set the ip_project_key in the langchain llm setup:

```python
llm = OpenAI(
    openai_api_key=os.environ["OPENAI_API_KEY"],
    model_kwargs={"ip_project_key": "my_project_key"},
)
```

## Credits

This package was created with Cookiecutter_ and the `audreyr/cookiecutter-pypackage`_ project template.

.. _Cookiecutter: https://github.com/audreyr/cookiecutter
.. _`audreyr/cookiecutter-pypackage`: https://github.com/audreyr/cookiecutter-pypackage


======= History =======

## 0.1.0 (2023-06-20)

-   First release on PyPI.

## 0.1.1 (2023-06-23)

-   add TemplateString helper and support for data / params

## 0.1.2 (2023-06-23)

-   add support for original template too

## 0.2.0 (2023-06-26)

- add explicit support for passing the "prompt template text"

## 0.3.0 (2023-06-28)

- add support for chat templates (as objects instead of arrays)

## 0.4.0 (2023-06-29)

- switch event reporting to be async / non-blocking

## 0.4.1 (2023-06-29)

- add utility for formatting langchain messages

## 0.4.2 (2023-06-29)

- remove stray breakpoint

## 0.4.3 (2023-06-30)

- pass along chat_id
- attempt to auto-convert langchain prompt templates

## 0.4.4 (2023-06-30)

- remove stray prints

## 0.5.0 (2023-07-06)

- Add langchain callbacks handlers

## 0.6.0 (2023-07-10)

- Handle duplicate callbacks, agents, etc

## 0.6.1 (2023-07-12)

- Fix prompt retrieval in deep chains

## 0.6.2 (2023-07-13)

- Handle cases where input values are not strings

## 0.6.3 (2023-07-18)

- Better support for server-generated event ids 
  (pre-llm sends event, post-llm re-uses the same id)
- more tests for different kinds of templates

## 0.6.4

- include temporary patched version of loads()

## 0.7.0

- breaking change: move im_openai.langchain_util to im_openai.langchain
- add support for injecting callbacks into all langchain calls using tracing hooks

## 0.7.1

- Pass along model params to the server
