#!/usr/bin/env python
# -*- coding: utf-8 -*--

# Copyright (c) 2023 Oracle and/or its affiliates.
# Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl/

import json
import os
import shutil
import tempfile
from collections import namedtuple
from typing import Dict, Tuple

import pandas
import requests
import yaml
from ads.common.auth import default_signer
from ads.model.datascience_model import DataScienceModel, ModelProvenanceMetadata
from ads.model.deployment.model_deployment import (
    ModelDeployment,
    ModelDeploymentCondaRuntime,
    ModelDeploymentContainerRuntime,
    ModelDeploymentInfrastructure,
)
from ads.model.model_metadata import MetadataCustomCategory, ModelCustomMetadata
from jinja2 import Environment, PackageLoader
from mlflow.artifacts import download_artifacts
from mlflow.client import MlflowClient
from mlflow.deployments import BaseDeploymentClient
from mlflow.store.artifact.utils.models import get_model_name_and_version
from tqdm.auto import tqdm

from oci_mlflow import logger
from oci_mlflow.telemetry_logging import Telemetry, telemetry
from oci_mlflow.utils import (
    DEFAULT_TAGS,
    build_and_publish_conda_pack,
    resolve_python_version,
)

CONFIGURATION_FILE_OPTION = "deploy-config-file"
MLFLOW_DEFAULT_PORT = 5000
MLFLOW_DEFAULT_CONDA_FILE = "conda.yaml"
DEFAULT_LOCAL_CONDA_DIR = "./conda"

CondaInfo = namedtuple(
    "CondaInfo", field_names=["uri", "python_version", "keep_local", "slug"]
)


class OCIModelDeploymentClient(BaseDeploymentClient):
    """
    MLFlow Plugin implementation for deploying models to OCI Data Science Service
    """

    def create_model(
        self,
        model_uri: str,
        infra_spec: Dict,
        name: str,
        version: str,
        model_local_dir: str,
        conda_uri: str,
        python_version: str,
        score_code: str = None,
    ) -> DataScienceModel:
        """
        Save model in the Data Science Model and then return the model object

        Parameters
        ----------
        model_uri: str
            URI pattern for models as published by MLFlow
        infra_spec: Dict
            A dictionary containing infrastructure section of Model Deployment spec.
        name: str
            Name of the model in MLFlow Model Registry
        version: str
            Version of the model in the MLFlow model registry
        model_local_dir: str
            Local directory where the MLFlow model is downloaded
        conda_uri: str
            conda pack path on object storage to use in model deployment. This parameter is required when runtime is conda.
        python_version: str
            required when runtime is conda
        score_code: str
            Path to score.py to override autogenerated score.py
        Returns
        -------
        DataScienceModel
            Instance of the DataScienceModel.
        """

        model_name = f"{name}_{version}"
        model_archive = f"{model_name}.zip"
        logger.debug(json.dumps(infra_spec, indent=2))

        client = MlflowClient()
        mflow_model = client.get_model_version(name, version)

        model = DataScienceModel(**infra_spec.get("spec", {}))
        model = (
            model.with_display_name(model_name)
            .with_freeform_tags(mlflow_model_uri=model_uri)
            .with_artifact(model_archive)
            .with_provenance_metadata(
                ModelProvenanceMetadata(
                    artifact_dir=os.path.abspath(os.path.dirname(model_archive))
                )
            )
        )

        model_metadata = ModelCustomMetadata()
        model_metadata.add(
            key="MLFlow Tracking Server",
            value=client.tracking_uri,
            category=MetadataCustomCategory.OTHER,
            description="MLFlow tracking server",
            replace=True,
        )
        model_metadata.add(
            key="Model URI",
            value=model_uri,
            category=MetadataCustomCategory.OTHER,
            description="MLFlow model uri",
            replace=True,
        )
        model_metadata.add(
            key="MLFlow Run ID",
            value=mflow_model.run_id,
            category=MetadataCustomCategory.OTHER,
            description="MLFlow Run ID",
            replace=True,
        )
        model_metadata.add(
            key="MLFlow Model Source",
            value=mflow_model.source,
            category=MetadataCustomCategory.OTHER,
            description="Artifact location of the model",
            replace=True,
        )

        model_metadata.add(
            key="MLFlow registry link",
            value=f"{client.tracking_uri}/#/models/{name}/versions/{version}",
            category=MetadataCustomCategory.OTHER,
            description="Link to the model on MLFlow registry",
            replace=True,
        )
        model = model.with_custom_metadata_list(model_metadata)
        logger.debug(model)

        # Generate runtime.yaml and score.py only when conda_uri is provided. Else assume container runtime.
        if conda_uri is not None:
            _env = Environment(loader=PackageLoader("oci_mlflow", "templates"))
            runtime_yaml_template = _env.get_template("runtime.yaml.jinja2")
            with open(os.path.join(model_local_dir, "runtime.yaml"), "w") as of:
                runtime_yaml = runtime_yaml_template.render(
                    conda_pack_uri=conda_uri, python_version=python_version
                )
                logger.info(
                    f"Generated runtime yaml with INFERENCE_ENV_PATH={conda_uri} and INFERENCE_PYTHON_VERSION = {python_version}"
                )
                logger.debug(runtime_yaml)
                of.write(runtime_yaml)

            # If score.py is provided by the user, use it else auto-generate
            if score_code:
                if os.path.basename(score_code) != "score.py":
                    err_msg = f"Expected file name `score.py` for scoreCode attribute, found: {score_code}"
                    logger.error(err_msg)
                    raise ValueError(err_msg)
                shutil.copy(score_code, os.path.join(model_local_dir, "score.py"))
                logger.info(f"Copied {score_code} to model artifact")
            else:
                scoring_template = _env.get_template("score.py.jinja2")
                with open(os.path.join(model_local_dir, "score.py"), "w") as of:
                    of.write(scoring_template.render())
                logger.info("Generated score.py")
        shutil.make_archive(model_name, format="zip", root_dir=model_local_dir)
        model = model.create()
        logger.info("Created model: ")
        logger.info(model)
        return model

    def conda_yaml_fetch(self, model_local_dir: str) -> str:
        """
        Returns the conda yaml file path

        Parameters
        ----------
        model_local_dir: str
            Local directory where the MLFlow model is downloaded

        Returns
        -------
        str
            conda yaml file path.
        """
        return os.path.join(model_local_dir, MLFLOW_DEFAULT_CONDA_FILE)

    def fetch_model_artifact(
        self, model_uri: str
    ) -> Tuple[str, str, tempfile.TemporaryDirectory]:
        """
        Download the artifact from MLFlow registry and return the name, version and download path

        Parameters
        ----------
        model_local_dir: str
            Local directory where the MLFlow model is downloaded

        Returns
        -------
        Tuple[str, str, tempfile.TemporaryDirectory]
            Returns a tuple of (name, version, download location)
        """

        logger.debug(f"The model URI is {model_uri}")
        dst_path = tempfile.TemporaryDirectory()
        client = MlflowClient()

        name, version = get_model_name_and_version(client, model_uri)

        artifact_uri = client.get_model_version_download_uri(name, version)
        logger.info(f"The artifact URI for the model: {model_uri} is {artifact_uri}")
        download_artifacts(model_uri, dst_path=dst_path.name)
        return name, version, dst_path

    def create_conda_environment(
        self, name: str, runtime: str, model_local_dir: str
    ) -> CondaInfo:
        """
        Create and publish conda pack for conda runtime if the URI is a dictionary instead of a path to conda pack

        Parameters
        ----------
        name: str
            name of the model
        runtime: str
            This could be conda or container
        model_local_dir: str
            Local directory where the MLFlow model is downloaded

        Returns
        -------
        ModelDeploymentEnvironmentConfigurationDetails
            Returns an instance of ModelDeploymentEnvironmentConfigurationDetails
        """
        runtime_spec = runtime["spec"]
        conda_uri = None
        python_version = None
        local_copy_remove = None
        slug = None
        conda_os_uri = None

        conda_spec = runtime["spec"]
        conda_uri = conda_spec.get("uri")
        logger.info(f"Conda URI is {conda_uri}")
        if not conda_uri:
            logger.error("uri attribute is mandatory under spec for conda runtime")
            raise (
                Exception(
                    f"Missing URI attribute under conda runtime inside :::TODO: Fix this"
                )
            )
        elif isinstance(conda_uri, dict):
            # TODO: The path has to be fetched from opctl config
            conda_pack_folder = conda_uri.get("localCondaDir", DEFAULT_LOCAL_CONDA_DIR)
            conda_yaml_file = self.conda_yaml_fetch(
                model_local_dir.name
                if isinstance(model_local_dir, tempfile.TemporaryDirectory)
                else model_local_dir
            )
            conda_os_uri = build_and_publish_conda_pack(
                name=f"{name}",
                version=conda_uri.get("version", "1"),
                environment_file=conda_yaml_file,
                conda_pack_folder=conda_pack_folder,
                gpu=conda_uri.get("gpu", False),
                overwrite=conda_uri.get("overwrite", False),
                conda_pack_os_prefix=conda_uri.get("destination"),
            )
            local_copy_remove = conda_uri.get("keepLocal", False)
            python_version = resolve_python_version(conda_yaml_file)
        elif isinstance(conda_uri, str):
            conda_os_uri = conda_uri
            python_version = conda_spec.get("pythonVersion")
            if not python_version:
                logger.error(
                    "Provide `pythonVersion` value under runtime > spec section for conda runtimes."
                )
                raise ValueError(
                    "Could not determine the python version of the provided conda environment."
                )

        return CondaInfo(conda_os_uri, python_version, local_copy_remove, slug)

    def _update_progress(self):
        """
        Method for updating the tqdm progress
        """
        self.step_cnt += 1
        self.progress.update(self.step_cnt)
        if self.step_cnt < len(self.steps):
            self.progress.set_description(self.steps[self.step_cnt], refresh=True)

    @telemetry("plugin=deployment&action=create")
    def create_deployment(
        self,
        name: str,
        model_uri: str,
        flavor: str = None,
        config: Dict = None,
        synchronous: bool = True,
        **kwargs,
    ):
        """
        Entry point for the deployment plugin. High level logic -

        * Download the model from MLFlow registry to local temp directory
        * If conda runtime,
            - then create and publish conda pack if the conda uri is a dictionary and not an object storage path.
            - generate score.py assuming python_function flavor
            - generate runtime.yaml with conda information
        * Else container runtime, nothing to do
        * Zip and upload model artifacts to OCI Data Science Model service
        * Create deployment instance with the model id

        """
        telemetry: Telemetry = kwargs.pop("telemetry", None)

        if flavor and flavor != "python_function":
            raise NotImplementedError(
                f"{flavor} is not implemented. Currently support flavor is python_function."
            )
        client = MlflowClient()

        self.steps = [
            "Downloading model from MLFlow registry",
            "Preparing Inference Environment",
            "Creating Data Science Model",
            "Deploying model",
        ]
        self.progress = tqdm(total=len(self.steps), desc=self.steps[0])
        self.step_cnt = 0
        spec = {}
        if CONFIGURATION_FILE_OPTION in config:
            with open(config[CONFIGURATION_FILE_OPTION]) as cf:
                spec = yaml.load(cf, Loader=yaml.SafeLoader)
        else:
            raise Exception(
                f"Require config yaml for deployment. Provide the yaml file for `{CONFIGURATION_FILE_OPTION}`"
            )

        runtime = spec["spec"][ModelDeployment.CONST_RUNTIME]

        # Download artifacts
        model_name, model_version, model_local_dir = self.fetch_model_artifact(
            model_uri
        )
        self._update_progress()

        # Prepare environment for deployment
        conda_info = CondaInfo(
            uri=None, python_version=None, keep_local=None, slug=None
        )
        if runtime["spec"].get("scoreCode"):
            logger.info(
                f'score.py path provided in the runtime: {runtime["spec"].get("scoreCode")}'
            )
        else:
            logger.info(f"Setting default score.py inside model artifact")

        if runtime["type"].lower() == ModelDeploymentCondaRuntime().type:
            conda_info = self.create_conda_environment(name, runtime, model_local_dir)
            if conda_info.uri:
                runtime["spec"]["uri"] = conda_info.uri
            if conda_info.python_version:
                runtime["spec"]["pythonVersion"] = conda_info.python_version
        self._update_progress()

        # Create model
        infrastructure = spec["spec"].get(ModelDeployment.CONST_INFRASTRUCTURE, {})
        model = self.create_model(
            model_uri,
            infrastructure,
            model_name,
            model_version,
            model_local_dir.name
            if isinstance(model_local_dir, tempfile.TemporaryDirectory)
            else model_local_dir,
            conda_info.uri,
            conda_info.python_version,
            score_code=runtime["spec"].get("scoreCode"),
        )
        client.set_model_version_tag(model_name, model_version, "model-ocid", model.id)
        if isinstance(model_local_dir, tempfile.TemporaryDirectory):
            model_local_dir.cleanup()  # Cleanup the downloaded model artifact
        # if conda_info.keep_local == False:  # ignore None case
        #     shutil.rmtree(os.path.join(CONDA_PACK_FOLDER, conda_info.slug))
        # Commenting this. Container creates all the files with root owner in ol8.

        self._update_progress()

        md = None
        try:
            runtime_clazz = (
                ModelDeploymentContainerRuntime
                if runtime["type"].lower() == ModelDeploymentContainerRuntime().type
                else ModelDeploymentCondaRuntime
            )
            md_runtime = runtime_clazz(**runtime).with_model_uri(model.id)

            md = (
                ModelDeployment()
                .with_display_name(name)
                .with_freeform_tags(**{**DEFAULT_TAGS, "mlflow_model_uri": model_uri})
                .with_infrastructure(ModelDeploymentInfrastructure(**infrastructure))
                .with_runtime(md_runtime)
            )

            if telemetry:
                telemetry.add(f"infrastructure={md.infrastructure.type}").add(
                    f"runtime={md.runtime.type}"
                )

            logger.info(
                f"Creating Model Deployment endpoint with configuration: \n {md}"
            )

            md.deploy(wait_for_completion=synchronous)

        except Exception as e:
            logger.error(f"Deployment failed: {e}")
            model.delete()
        self._update_progress()

        model_deployment_ocid = ""
        model_deployment_endpoint = ""

        if md:
            model_deployment_ocid = md.model_deployment_id
            model_deployment_endpoint = md.url
            client.set_model_version_tag(
                model_name,
                model_version,
                "model-deployment-ocid",
                model_deployment_ocid,
            )
            client.set_model_version_tag(
                model_name,
                model_version,
                "model-deployment-endpoint",
                model_deployment_endpoint,
            )

            logger.info(
                (
                    f"Model deployed successfully. The model deployment ocid is {model_deployment_ocid}."
                    f"The inference endpoint is {model_deployment_endpoint}. For more details -"
                    f"https://cloud.oracle.com/data-science/model-deployments/{model_deployment_ocid}"
                )
            )
        return {
            "flavor": flavor if flavor else "python_function",
            "name": model_deployment_ocid,
            "url": model_deployment_endpoint,
        }

    def delete_deployment(self, name, config=None, endpoint=None):
        return ModelDeployment.from_id(name).delete()

    def get_deployment(self, name, endpoint=None):
        return {"model": ModelDeployment.from_id(name)}

    def list_deployments(self, endpoint=None):
        # TODO: use environment variable for compartment id and project id.
        raise NotImplementedError()

    def predict(self, deployment_name=None, inputs=None, endpoint=None):
        payload = inputs
        if isinstance(inputs, pandas.core.frame.DataFrame):
            payload = json.loads(inputs.to_json())
        url = ModelDeployment.from_id(deployment_name).url

        data = requests.post(
            f"{url}/predict", auth=default_signer()["signer"], json=payload
        )
        return pandas.DataFrame(data=data)

    def update_deployment(
        self, name, model_uri=None, flavor=None, config=None, endpoint=None
    ):
        md = ModelDeployment.from_id(name)

        spec = {}

        if CONFIGURATION_FILE_OPTION in config:
            with open(config[CONFIGURATION_FILE_OPTION]) as cf:
                spec = yaml.load(cf, Loader=yaml.SafeLoader)
        else:
            raise Exception(
                f"Require config yaml for deployment. Provide the yaml file for `{CONFIGURATION_FILE_OPTION}`"
            )

        # Create model
        infrastructure = spec["spec"].get(md.CONST_INFRASTRUCTURE, {})
        current_infra = md.infrastructure.to_dict()
        updated_infra = {**current_infra, **infrastructure}
        try:
            md.with_infrastructure(ModelDeploymentInfrastructure(**updated_infra))
            if spec["spec"].get(md.CONST_DISPLAY_NAME):
                md.with_display_name(spec["spec"].get(md.CONST_DISPLAY_NAME))
            logger.debug(
                f"Update Model deployment with following configuration: \n{md}"
            )
            md.update()
            logger.info(
                f"Model Deployment {name} updated successfully with configuration: \n {md}"
            )
        except Exception as e:
            logger.error(f"Error updating model deployment {name}: \n\t {e}")

        return {
            "flavor": flavor if flavor else "python_function",
            "name": md.model_deployment_id,
            "url": md.url,
        }


def target_help(**kwargs):
    print(
        """

Deploy
======

To deploy use `--config deploy-config-file=<config yaml file>` option. Refer deployment spec in `https://github.com/oracle/accelerated-data-science/spec/deploymment.yaml`. For more examples for deployment check - `https://github.com/oracle/oci_mlflow/examples/deployments`

Eg. `mlflow deployments create --name mytestmodel -m models:/mymodel/model-version -t oci-datascience --config deploy-config-file=./deployment_config.yaml`

Delete
======

To delete a model deployment, the provide the ocid for name parameter.

Eg. `mlflow deployments delete -t oci-datascience --name ocid1.datasciencemodeldeployment.oc1.xxx.xxxxxxxxxxxx`

Get Deployment
==============

To get model deployment details, provide the ocid for name parameter.

Eg. `mlflow deployments get -t oci-datascience --name ocid1.datasciencemodeldeployment.oc1.xxx.xxxxxxxxxxxx`

Prediction
==========

To fetch prediction, provide the OCID for name parameter.

Eg. `mlflow deployments predict --name ocid1.datasciencemodeldeployment.oc1.xxx.xxxxxxxxxxxx -t oci-datascience -I ./input.json`

Update
======

To update model deployment configuration, provide the OCID for name parameter and use `--config deploy-config-file=<config yaml file>` option.

Note: You may not be able to change all the configuration in one pass. Check `Editing Model Deployments` section in https://docs.oracle.com/en-us/iaas/data-science/using/model_dep_manage.htm for more details.

Eg. mlflow deployments update --name ocid1.datasciencemodeldeployment.oc1.xxx.xxxxxxxxxxxx -t oci-datascience --config deploy-config-file=./deployment_update_config.yaml

"""
    )


def run_local(**kwargs):
    raise NotImplementedError()
