[metadata]
name = crfm-helm
version = 0.2.3
author = Stanford CRFM
author_email = contact-crfm@stanford.edu
description = Benchmark for language models
long_description = Benchmark for language models
keywords = language models benchmarking
license = Apache License 2.0
classifiers = 
	Programming Language :: Python :: 3 :: Only
	Programming Language :: Python :: 3.8
	License :: OSI Approved :: Apache Software License
url = https://github.com/stanford-crfm/helm

[options]
python_requires = ~=3.8
package_dir = 
	=src
packages = find:
zip_safe = False
include_package_data = True
install_requires = 
	cattrs~=22.2.0
	dacite~=1.6.0
	importlib-resources~=5.10.0
	Mako~=1.2.3
	numpy~=1.23.3
	pyhocon~=0.3.59
	retrying~=1.3.4
	simple-slurm~=0.2.6  # For slurm_jobs
	spacy~=3.5.3
	tqdm~=4.64.1
	zstandard~=0.18.0
	sqlitedict~=1.7.0
	protobuf~=3.20.2  # Can't use 4.21.0 due to backward incompatibility
	pymongo~=4.2.0
	
	datasets~=2.5.2
	pyarrow~=11.0.0  # Pinned transitive dependency for datasets; workaround for #1026
	jsonlines~=3.1.0  # Not really needed
	
	nltk~=3.7
	pyext~=0.7
	rouge-score~=0.1.2
	scipy~=1.9.1
	uncertainty-calibration~=0.1.3
	scikit-learn~=1.1.2
	
	bottle~=0.12.23
	gunicorn~=20.1.0
	
	gdown~=4.4.0  # For opinions_qa_scenario
	sympy~=1.11.1  # For numeracy_scenario
	xlrd~=2.0.1  # For ice_scenario: used by pandas.read_excel
	
	aleph-alpha-client~=2.14.0
	anthropic~=0.2.5
	icetk~=0.0.4  # for ice_tokenizer_client
	openai~=0.27.8
	revChatGPT~=0.1.1
	sentencepiece~=0.1.97  # For palmyra_client and yalm_tokenizer
	tiktoken~=0.3.3  # for openai_client
	tokenizers~=0.13.3  # for aleph_alpha_client
	websocket-client~=1.3.2  # For Anthropic (Legacy stanford-online-all-v4-s3)
	
	transformers~=4.28.1  # For anthropic_client, huggingface_client, huggingface_tokenizer, test_openai_token_cost_estimator, model_summac (via summarization_metrics)
	torch~=1.12.1  # For huggingface_client, yalm_tokenizer, model_summac (via summarization_metrics)
	torchvision~=0.13.1  # For huggingface_client, yalm_tokenizer, model_summac (via summarization_metrics)
	
	google-api-python-client~=2.64.0  # For perspective_api_client via toxicity_metrics
	numba~=0.56.4  # For copyright_metrics
	pytrec_eval==0.5  # For ranking_metrics
	sacrebleu~=2.2.1  # For disinformation_metrics, machine_translation_metrics
	summ-eval~=0.892  # For summarization_metrics
	
	scaleapi~=2.13.0
	surge-api~=1.1.0
	
	colorcet~=3.0.1
	matplotlib~=3.6.0
	seaborn~=0.11.0

[options.entry_points]
console_scripts = 
	helm-run = helm.benchmark.run:main
	helm-summarize = helm.benchmark.presentation.summarize:main
	helm-server = helm.benchmark.server:main
	helm-create-plots = helm.benchmark.presentation.create_plots:main
	crfm-proxy-server = helm.proxy.server:main
	crfm-proxy-cli = helm.proxy.cli:main

[options.packages.find]
where = src
exclude = 
	tests*

[flake8]
max-line-length = 120
exclude = venv/*
ignore = E203,E231,E731,W503,W605

[mypy]
ignore_missing_imports = True

[tool:pytest]
addopts = 
	-m 'not models'
markers = 
	models

[egg_info]
tag_build = 
tag_date = 0

