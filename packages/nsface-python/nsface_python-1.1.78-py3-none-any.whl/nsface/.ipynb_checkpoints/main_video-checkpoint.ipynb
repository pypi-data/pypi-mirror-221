{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from model_zoo.get_models import get_detection_model,get_landmark_model, get_ageGender_model,get_recognition_model\n",
    "from face.get_result import *\n",
    "from utils.util_common import draw_result,draw_result_sim\n",
    "from data.image import read_image,read_video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model list\n",
    "#### detection\n",
    "1. scrfd : '/data/notebook/NAS/PTAS_Shared/resource/model/face/detection/scrfd_10g_bnkps.onnx'\n",
    "2. dlib  : '/data/notebook/NAS/Gender-Age/source/SSR-Pytorch/datasets/shape_predictor_68_face_landmarks.dat'\n",
    "\n",
    "#### landmark\n",
    "1. 3ddfa : '/data/notebook/NAS/PTAS_Shared/resource/model/face/landmark/3ddfa_v2.onnx'\n",
    "\n",
    "#### recognition\n",
    "1. arcface r50 torch : \"/data/notebook/insightface/recognition/arcface_torch/backbones/recog_arc_r50.pth\"\n",
    "2. arcface r50 onnx  : \"/data/notebook/NAS/PTAS_Shared/resource/model/face/embedding/res50_arcface_20220210.onnx\"\n",
    "\n",
    "##### genderage\n",
    "1. arcface_cmt torch : '/data/notebook/NAS/PTAS_Shared/resource/model/face/agender/cmt_r50_arcface_best_20220119-2.pt'\n",
    "2. arcface_cmt onnx  : \"/data/notebook/NAS/PTAS_Shared/resource/model/face/agender/cmt_r50_arcface_best_20220119-2.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detection\n",
    "\n",
    "detection_name='scrfd'\n",
    "detection_path = '/data/notebook/NAS/PTAS_Shared/resource/model/face/detection/scrfd_10g_bnkps.onnx'\n",
    "#detection_name='dlib'\n",
    "#detection_path = '/data/notebook/NAS/Gender-Age/source/SSR-Pytorch/datasets/shape_predictor_68_face_landmarks.dat'\n",
    "detection_thresh = 0.5\n",
    "detection_height_min=60\n",
    "\n",
    "\n",
    "# landmark (None is '')\n",
    "landmark_name='3ddfa'\n",
    "landmark_path = '/data/notebook/NAS/PTAS_Shared/resource/model/face/landmark/3ddfa_v2.onnx'\n",
    "\n",
    "\n",
    "# recognition\n",
    "recog_name = \"arcface\"\n",
    "#recog_path = \"/data/notebook/insightface/recognition/arcface_torch/backbones/recog_arc_r50.pth\"\n",
    "recog_path = \"/data/notebook/NAS/PTAS_Shared/resource/model/face/embedding/res50_arcface_20220210.onnx\"\n",
    "recog_out_size=112\n",
    "recog_num_features=512\n",
    "recog_network='r50'\n",
    "recog_fp16=True\n",
    "\n",
    "\n",
    "# ageGender\n",
    "ag_name='arcface_cmt'\n",
    "#ag_path = '/data/notebook/NAS/PTAS_Shared/resource/model/face/agender/cmt_r50_arcface_best_20220119-2.pt'\n",
    "age_path = \"/data/notebook/NAS/PTAS_Shared/resource/model/face/agender/cmt_r50_arcface_best_20220119-2.onnx\"\n",
    "ag_out_size=112\n",
    "ag_num_features=512\n",
    "ag_network='r50'\n",
    "\n",
    "\n",
    "# draw\n",
    "sim_thresh = 0.4\n",
    "max_sim = False\n",
    "all_draw = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # max-size one box\n",
    "    if max_flag and det.shape[0]>1:\n",
    "        det_sizes=[]\n",
    "        for di,det_ in enumerate(det):\n",
    "            if model_name=='scrfd':\n",
    "                score = det_[4]\n",
    "            elif model_name=='dlib':\n",
    "                score = scores[di]\n",
    "            if score < thresh:\n",
    "                det_sizes.append(0)\n",
    "                continue\n",
    "            det_size = (det_[2]-det_[0])*(det_[3]-det_[1])\n",
    "            det_sizes.append(det_size)\n",
    "        max_idx = np.argmax(det_sizes)\n",
    "\n",
    "        det = det[max_idx]\n",
    "        kps = kpss[max_idx]\n",
    "        if model_name=='scrfd':\n",
    "            bbox = det[:4]\n",
    "            bbox = get_new_bbox(bbox,pos_x,pos_y,det_scale)\n",
    "            kps = get_new_kps(kps,pos_x,pos_y,det_scale)\n",
    "\n",
    "        elif model_name=='dlib':\n",
    "            bbox = det[max_idx]\n",
    "\n",
    "        face = Face(bbox=bbox,land5=kps)\n",
    "        if height_min==0 or (bbox[3]-bbox[1])>height_min:\n",
    "            ret.append(face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_dirs = \"ref_images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"/data/notebook/NAS/Gender-Age/test_data/test_japan_office/japan_office.mp4\"\n",
    "input_path = \"/data/notebook/NAS/Gender-Age/test_data/test_japan_office/俗姓分析demo_2022.03.08.mp4\"\n",
    "\n",
    "input_name = input_path.split(\"/\")[-1].split(\".\")[0]\n",
    "input_format = input_path.split(\".\")[1]\n",
    "\n",
    "save_flag=True\n",
    "\n",
    "if save_flag:\n",
    "    #save_name = \"{}_result_sim.{}\".format(input_name,input_format)\n",
    "    save_name = \"demo_2022-03-08_sim_onnx.mp4\"\n",
    "    save_result = os.path.join(\"./save_result/\",save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./save_result/demo_2022-03-08_sim_onnx.mp4\n"
     ]
    }
   ],
   "source": [
    "print(save_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faces [dict,dict,dict...]\n",
    "# dict_keys(['bbox', 'land5', 'aimg', 'gender', 'age'])\n",
    "# bbox: bounding box\n",
    "# land5: landmark 5 points (eye left, eye right, nose, mouth left, mouth right)\n",
    "# aimg: aligned img\n",
    "# gender, age: predicted gender and age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scrfd onnx loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py:353: UserWarning: Deprecation warning. This ORT build has ['CUDAExecutionProvider', 'CPUExecutionProvider'] enabled. The next release (ORT 1.10) will require explicitly setting the providers parameter (as opposed to the current behavior of providers getting set/registered by default based on the build flags) when instantiating InferenceSession.For example, onnxruntime.InferenceSession(..., providers=[\"CUDAExecutionProvider\"], ...)\n",
      "  \"based on the build flags) when instantiating InferenceSession.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3ddfa onnx loaded\n",
      "arcface onnx loaded\n",
      "arcface_cmt pt loaded\n"
     ]
    }
   ],
   "source": [
    "# 1. load models\n",
    "detection_model = get_detection_model(detection_name,detection_path)\n",
    "landmark_model = get_landmark_model(landmark_name,landmark_path)\n",
    "recog_model = get_recognition_model(recog_name,recog_path,out_size=recog_out_size,num_features=recog_num_features,network=recog_network,fp16=recog_fp16)\n",
    "ag_model = get_ageGender_model(ag_name, ag_path,ag_out_size,ag_num_features,ag_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ref features\n",
    "refs={}\n",
    "\n",
    "for img_name in os.listdir(ref_dirs):\n",
    "    pname = img_name.split(\".\")[0]\n",
    "    img_format = img_name.split(\".\")[1]\n",
    "    if not img_format in ['jpg','JPG','png']:\n",
    "        continue\n",
    "    img_path = os.path.join(ref_dirs,img_name)\n",
    "    img_feat = recog_model.get_ref(img_path,True)\n",
    "    refs[pname]=img_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Read video(default bgr)\n",
    "\n",
    "vid, video_frame_cnt, video_width, video_height, video_fps = read_video(input_path)\n",
    "\n",
    "if save_flag:\n",
    "    fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "    videoWriter = cv2.VideoWriter(save_result, fourcc, video_fps, (video_width, video_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save result: ./save_result/demo_2022-03-08_sim_onnx.mp4\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "limit_frames=3000\n",
    "\n",
    "while True:\n",
    "    ret, frame = vid.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # 3. Detect faces\n",
    "    faces = get_detection(detection_name,detection_model,frame,thresh=detection_thresh,height_min=detection_height_min)\n",
    "    \n",
    "    # 3-1. Landmark faces\n",
    "    if landmark_name and landmark_path:\n",
    "        faces, param_lst, roi_box_lst = get_landmark(landmark_name,landmark_model,frame,faces)\n",
    "        \n",
    "    # 4. embedding faces\n",
    "    faces = get_features(recog_name,recog_model,frame,faces,convert=True)\n",
    "        \n",
    "    # 5. ageGender, if need RGB input, convert=True\n",
    "    faces = get_ageGender(ag_name,ag_model,frame,faces,convert=False)\n",
    "    \n",
    "    # save draw bbox and predict image\n",
    "    # if put text (predict age gender) ga=True \n",
    "    # if need bgr to rgb, convert True\n",
    "    #dimg = draw_result(frame,faces,ga=True,convert=False)\n",
    "    dimg = draw_result_sim(frame,faces,refs,sim_thresh=sim_thresh,ga=True,convert=False,all_draw=all_draw)\n",
    "    \n",
    "    if save_flag:\n",
    "        videoWriter.write(dimg)\n",
    "    \n",
    "    if limit_frames==c:\n",
    "        break\n",
    "    c+=1\n",
    "vid.release()\n",
    "if save_flag:\n",
    "    videoWriter.release()\n",
    "    print(\"Save result:\",save_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
